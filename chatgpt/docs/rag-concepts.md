# üß† RAG & Vector DB Concepts

This doc is a conceptual cheat-sheet for new learners.

---

## üß± What is a Vector Database?

- Stores data as **vectors** (= numeric representations of text/images).
- Similar items have **similar vectors**.
- ChromaDB is an open-source vector DB.

**Why not just use SQL?**

- SQL is great for exact matching.
- Semantic search (e.g., "find things about space stations") needs **meaning-based** matching.
- Embeddings let us measure meaning-distance.

---

## üß¨ What are Embeddings?

- A **mathematical representation** of text in high-dimensional space.
- Example:
  - "cat" and "kitten" ‚Üí close vectors.
  - "rocket" and "finance" ‚Üí far apart.
- Generated by an **embedding model** (often a neural network).

In this project:

- ChromaDB handles embeddings internally when you call `collection.add(...)`.

---

## üîÅ What is RAG (Retrieval-Augmented Generation)?

**Goal**

- Make LLMs answer using **your data**, not just what they were trained on.

**Pattern**

1. User asks a question.
2. System searches a vector DB for relevant docs.
3. System builds a prompt like:
   - "Answer the question using ONLY the context below: ..."
4. LLM answers with grounded information.

**Benefits**

- Current & private data.
- Reduced hallucinations.
- Transparent knowledge sources.

---

## üß© How This Project Demonstrates RAG

- **Ingestion**:
  - Upload document ‚Üí Extract text ‚Üí Store in S3 ‚Üí Embed into Chroma.
- **Retrieval**:
  - Query endpoint `/query` uses Chroma's semantic search.
  - You can plug the `/query` results into any LLM.

**Next Steps for You**

- Build a small Python or Node service that:
  1. Accepts a user question.
  2. Calls `/query` to get context.
  3. Calls your favorite LLM API with that context.
  4. Returns the combined answer.

This project gives you the **data layer** of RAG.
